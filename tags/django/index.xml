<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Django on Lucas Roesler</title>
    <link>http://lucasroesler.com/tags/django/index.xml</link>
    <description>Recent content in Django on Lucas Roesler</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://lucasroesler.com/tags/django/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Delete or not to delete</title>
      <link>http://lucasroesler.com/2017/04/delete-or-not-to-delete/</link>
      <pubDate>Sun, 02 Apr 2017 00:00:00 -0700</pubDate>
      
      <guid>http://lucasroesler.com/2017/04/delete-or-not-to-delete/</guid>
      <description>&lt;p&gt;Should you remove data from the database or simply mark it as deleted?
At Teem we have a lot of data that we need to manage and often &amp;ldquo;physically&amp;rdquo;
deleting the data from disk can be problematic. Either the users simply wants
to undelete something or the deletion would cause problems for a log. The
generic solution to this problem is to soft delete/archive the data by adding
a &lt;code&gt;deleted_at&lt;/code&gt; timestamp field to the table and then filter all queries to
hide rows that have been marked as deleted.&lt;/p&gt;

&lt;!-- more /--&gt;

&lt;p&gt;This simple approach can take you a long way but doesn&amp;rsquo;t fully cover all the
required scenarios. More specifically, what do you do with related objects?
About a year ago, I realized that we really needed to address the
entire problem. There are several pieces of data that we never want to delete,
e.g. devices and visitors among others. At the time I couldn&amp;rsquo;t find a really
good solution so I wrote my own:
&lt;a href=&#34;https://github.com/LucasRoesler/django-archive-mixin&#34;&gt;django-archive-mixin&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As of today, there are at least two other options that cover this situation as
well and you should checkout too:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/scoursen/django-softdelete&#34;&gt;django-softdelete&lt;/a&gt;, and&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/makinacorpus/django-safedelete&#34;&gt;django-safedelete&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The biggest distinction between our project and the ones above is that I tried
to stay as close to the delete logic used in the original Django source code as
possible.  In particular, this means  mimicking Django&amp;rsquo;s &lt;code&gt;collector&lt;/code&gt; code that
implements the ORM cascade delete.  It is interesting to note that Django does
not rely on the database&amp;rsquo;s cascade functionality but instead manages the delete
process itself. The benefit of doing this is that you can then specify behavior
at delete time via the &lt;code&gt;on_delete&lt;/code&gt; argument to the model field.  For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Car(models.Model):
    manufacturer = models.ForeignKey(
        &#39;production.Manufacturer&#39;,
        blank=True, null=True
        on_delete=models.SET_NULL,
    )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;will cause the manufacturer field to be set to &lt;code&gt;None&lt;/code&gt; when you delete the
related manufacture. The default behavior would be to delete the car instance
as well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.djangoproject.com/en/1.10/ref/models/fields/#django.db.models.ForeignKey.on_delete&#34;&gt;Django provides 6 on_delete
options&lt;/a&gt;:
&lt;code&gt;CASCADE&lt;/code&gt;, &lt;code&gt;PROTECT&lt;/code&gt;, &lt;code&gt;SET_NULL&lt;/code&gt;, &lt;code&gt;SET_DEFAULT&lt;/code&gt;, &lt;code&gt;SET()&lt;/code&gt;, and &lt;code&gt;DO_NOTHING&lt;/code&gt;.
At delete, the Django &lt;code&gt;collector&lt;/code&gt; crawls the relationships and
buckets each object found into different lists depending on the &lt;code&gt;on_delete&lt;/code&gt;
configuration for that specific relationship.  &lt;code&gt;CASCADE&lt;/code&gt; puts the object in a
bucket to be deleted, &lt;code&gt;PROTECT&lt;/code&gt; will cause an exception to be thrown,
&lt;code&gt;SET_NULL&lt;/code&gt;, &lt;code&gt;SET_DEFAULT&lt;/code&gt;, and &lt;code&gt;SET()&lt;/code&gt; each cause and update to that instance,
and &lt;code&gt;DO_NOTHING&lt;/code&gt; is a no-op. Once I understood this process, I decided to
piggyback on the process and add an additional piece of logic to put more
objects into the update bucket. Essentially, I allow Django to do all of the
collection for me and then I go through the list of objects to inspect if it
should be archived instead, if it is, I move it to the update bucket and move
on.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def cascade_archive(inst_or_qs, using, keep_parents=False):
    &amp;quot;&amp;quot;&amp;quot;
    Return collector instance that has marked ArchiveMixin instances for
    archive (i.e. update) instead of actual delete.

    Arguments:
        inst_or_qs (models.Model or models.QuerySet): the instance(s) that
            are to be deleted.
        using (db connection/router): the db to delete from.
        keep_parents (bool): defaults to False.  Determine if cascade is true.

    Returns:
        models.deletion.Collector: this is a standard Collector instance but
            the ArchiveMixin instances are in the fields for update list.
    &amp;quot;&amp;quot;&amp;quot;
    from .mixins import ArchiveMixin

    if not isinstance(inst_or_qs, models.QuerySet):
        instances = [inst_or_qs]
    else:
        instances = inst_or_qs

    deleted_ts = timezone.now()

    # The collector will iteratively crawl the relationships and
    # create a list of models and instances that are connected to
    # this instance.
    collector = models.deletion.Collector(using=using)
    if StrictVersion(django.get_version()) &amp;lt; StrictVersion(&#39;1.9.0&#39;):
        collector.collect(instances)
    else:
        collector.collect(instances, keep_parents=keep_parents)
    collector.sort()

    for model, instances in collector.data.iteritems():
        # remove archive mixin models from the delete list and put
        # them in the update list.  If we do this, we can just call
        # the collector.delete method.
        inst_list = list(instances)

        if issubclass(model, ArchiveMixin):
            deleted_on_field = get_field_by_name(model, &#39;deleted_on&#39;)
            collector.add_field_update(
                deleted_on_field, deleted_ts, inst_list)

            del collector.data[model]

    for i, qs in enumerate(collector.fast_deletes):
        # make sure that we do archive on fast deletable models as
        # well.
        model = qs.model

        if issubclass(model, ArchiveMixin):
            deleted_on_field = get_field_by_name(model, &#39;deleted_on&#39;)
            collector.add_field_update(deleted_on_field, deleted_ts, qs)

            collector.fast_deletes[i] = qs.none()

    return collector
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What I love about this logic is that is is a fairly small change to how
deletion works while also being fairly low-level enough that it covers all of
the deletion cases that the Django ORM handles.&lt;/p&gt;

&lt;p&gt;We have been using this mixin for about a year now with no hiccups. It works as
expected and hasn&amp;rsquo;t really needed much attention. If you are using Django and
have been looking for a safe delete/archive utility
&lt;a href=&#34;https://github.com/LucasRoesler/django-archive-mixin&#34;&gt;check it out&lt;/a&gt; and let me
know what you think.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Zero downtime deploys: A tale of Django migrations</title>
      <link>http://lucasroesler.com/2017/02/zero-downtime-deploys-a-tale-of-django-migrations/</link>
      <pubDate>Mon, 20 Feb 2017 00:00:00 -0700</pubDate>
      
      <guid>http://lucasroesler.com/2017/02/zero-downtime-deploys-a-tale-of-django-migrations/</guid>
      <description>&lt;p&gt;At Teem, we aim for zero down-time deploys; so, one of the most
important things we must validate is that things will not break mid-deploy!&lt;/p&gt;

&lt;p&gt;The most sensitive step of the deploy process is the changes to our database.
Prior to the automation I am about to describe, validation of the database
migrations required specialized knowledge about Postgres, the changes to the
application model, load on the database for that model, and a bit of general
experience. This obviously slows down reviews and subsequently deploys. Worse,
it was simply too easy to miss problem migrations when depending on only peer
reviews. To make our lives easier we created a series of validation checks to
ensure that each database migration will be backwards compatible.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;the-what&#34;&gt;The what&lt;/h2&gt;

&lt;p&gt;The checks I am going to describe are simply a sequence of regex
that we run on the migrations in the changelog. The process looks, roughly,
like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Using git, generate a list of new migrations in this release,&lt;/li&gt;
&lt;li&gt;Using Django&amp;rsquo;s &lt;code&gt;sqlmigrate&lt;/code&gt; manage command, generate the SQL for each
migration,&lt;/li&gt;
&lt;li&gt;Run a sequence of regex on each SQL command,&lt;/li&gt;
&lt;li&gt;Report the issues,&lt;/li&gt;
&lt;li&gt;Profit!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We do this in a python script we internally call &lt;code&gt;octoeb&lt;/code&gt; which uses
&lt;a href=&#34;http://click.pocoo.org/5/&#34;&gt;Click&lt;/a&gt; to create a commandline
interface.  So, I can get a changelog along with an audit of the migrations
in my current branch using:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ocotoeb changelog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I won&amp;rsquo;t describe the specific python code, instead I will give you equivalent bash commands
that you can run in your CLI and a simple description of the regex that we
are using. This will give you all the pieces you need to build a similar script
in your favorite language.&lt;/p&gt;

&lt;h2 id=&#34;the-why&#34;&gt;The why&lt;/h2&gt;

&lt;p&gt;The basic goal is to ensure that any applied migrations are backwards
compatible with the model definitions in the currently deployed release. This
is a requirement because our current deployment process looks like:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;pull the new release code to a single server,&lt;/li&gt;
&lt;li&gt;run the migrations,&lt;/li&gt;
&lt;li&gt;restart the application,&lt;/li&gt;
&lt;li&gt;check the application status,&lt;/li&gt;
&lt;li&gt;slowly roll the code to the rest of the servers.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As a result, during a deploy we have a mix of old model definitions and new
model definitions running simultaneously.  This means that the database must
except both the old and the new for a short period of time and that any change we make
to the database should not lock up an entire table.&lt;/p&gt;

&lt;p&gt;Probably the most common change we often want to make is simply adding a new
column to an existing model.  This can present several issues.  First, your new
column should not set a default.  In postgres, adding a column with a default will
lock the table while it rewrites the existing rows with the default.  This can
easily be avoided by adding the column first without the default, then adding
the default, followed by a future backfill on the existing rows.  This will
create the column and all future rows will have the default.&lt;/p&gt;

&lt;p&gt;Implicit in the above recommendation is that all new columns must be nullable.
You can not add a column without a default unless you allow null. Additionally,
while the old models are running against the new table definitions, the app
will set a value for that column, so it must either have a default or allow
null otherwise Postgres will throw an error.&lt;/p&gt;

&lt;p&gt;Finally, the other change that we need to watch for is removing columns. This
is a multi-step process. If you drop a column while the old models are still
active you will get two possible errors (1) when Django tries to select on
that column that no longer exists (which it will because it always explicitly
names the columns selected) or (2) attempting to insert data to a column that
doesn&amp;rsquo;t exist anymore. To actually handle this type of model change you must
deploy the model change prior to running the migration.  In our process, that
means you must commit the model change in a release separate from the database
migration.&lt;/p&gt;

&lt;p&gt;There are certainly other cases to consider, but we have found these 3 cases to
cover the vast majority of our migration concerns. Having put these checks into
place, we rarely have any issues with database migrations during deploy.&lt;/p&gt;

&lt;h2 id=&#34;the-how&#34;&gt;The how&lt;/h2&gt;

&lt;h3 id=&#34;getting-your-list-of-migrations&#34;&gt;Getting your list of migrations&lt;/h3&gt;

&lt;p&gt;To find the new migrations you can run the following command&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git log --name-status master.. | grep -e  &amp;quot;^[MA].*migrations.*&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Breaking this down, &lt;code&gt;git log --name-status master..&lt;/code&gt; will print a log of the
commits and the file changes in each commit between &lt;code&gt;master&lt;/code&gt; and the current
&lt;code&gt;HEAD&lt;/code&gt;.  The &lt;code&gt;grep&lt;/code&gt; returns only those lines that start with &lt;code&gt;A&lt;/code&gt; or &lt;code&gt;M&lt;/code&gt; and
also contains the work &lt;code&gt;migrations&lt;/code&gt;.  These are all of the new or modified
migration files.  It will return something like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;A	apps/accounts/migrations/0019_auto_20170126_1830.py
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;getting-your-sql&#34;&gt;Getting your SQL&lt;/h3&gt;

&lt;p&gt;Once you have the list of migration files that you need to check, we need to
get the actual SQL that is going to be run by Django.  Fortunately, Django
provides a simple command to get this SQL,
&lt;a href=&#34;https://docs.djangoproject.com/en/1.10/ref/django-admin/#sqlmigrate&#34;&gt;&lt;code&gt;sqlmigrate&lt;/code&gt;&lt;/a&gt;.
For example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;django-admin sqlmigrate account 0002
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;will print the sql for the second migration in the &lt;code&gt;accounts&lt;/code&gt; app. In the
pervious section the ouput contains all of the information that we need.
Specifically, with a command like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git log --name-status master.. | grep -e  &amp;quot;^[AM].*migrations.*&amp;quot; | cut -d / -f 2,4 | cut -d . -f 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;we would get back exactly the list of apps and the migration name for each
migration that we need to check&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;accounts/0017_auto_20170126_1342
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This still isn&amp;rsquo;t quite what we need.  At the end of the day the following
command will generate the SQL for you&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git log --name-status master.. | grep -e  &amp;quot;^[AM].*migrations.*&amp;quot; | cut -d / -f 2,4 | cut -d . -f 1 | awk -F&amp;quot;/&amp;quot; &#39;{ print $1,$2}&#39; | xargs -t -L 1 django-admin sqlmigrate
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are using python for our scripting, so my script is actually a bit
different, using the regex &lt;code&gt;apps/((.*)/migrations/(\d+[0-9a-z_]*))\.py&lt;/code&gt; and a
combination of a for loop and subprocess to generate the SQL.&lt;/p&gt;

&lt;h3 id=&#34;regex-magic&#34;&gt;Regex magic&lt;/h3&gt;

&lt;p&gt;Now that we have the actual SQL that needs to be tested, it is simply a matter
of running a few regex tests. We have 3 core tests that we run:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;check_for_not_null&lt;/code&gt; which we test using &lt;code&gt;/NOT NULL/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;check_for_dropped_columns&lt;/code&gt; which we test using &lt;code&gt;/DROP COLUMN/&lt;/code&gt;,
and&lt;/li&gt;
&lt;li&gt;&lt;code&gt;check_for_add_with_default&lt;/code&gt; which we test using &lt;code&gt;/ADD COLUMN .* DEFAULT/&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For each migration, we test those 3 regex and alert if they have any matches.
As I mentioned earlier, there are certainly other cases that could be
considered. Let me know if there are some additional checks I should add.
Since we have implemented these checkes, I can&amp;rsquo;t remember the last time we had
a migration issue during a deploy so they seem to cover most of the use cases
we run into.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Postgres Joins and Django Querysets</title>
      <link>http://lucasroesler.com/2017/02/postgres-joins-and-django-querysets/</link>
      <pubDate>Mon, 06 Feb 2017 20:00:00 +0000</pubDate>
      
      <guid>http://lucasroesler.com/2017/02/postgres-joins-and-django-querysets/</guid>
      <description>&lt;p&gt;How do we build a fast API against database models with foreign keys and many-
to-many relationships?  If you do nothing you get what I call the &amp;ldquo;waterfall of
doom&amp;rdquo;. At some point in the past someone told me or I read that &amp;ldquo;joins are
effectively free in Postgres&amp;rdquo;.  While this might be somewhat true when you are
writing all of the SQL and can control every part of your query; I have recently
found that when the database gets big enough and you are using the Django ORM,
joins aren&amp;rsquo;t free and less can be more!&lt;/p&gt;

&lt;p&gt;Warning, I am not a DBA and mileage may vary.&lt;/p&gt;

&lt;!-- more /--&gt;

&lt;p&gt;At &lt;a href=&#34;https://teem.com&#34;&gt;Teem&lt;/a&gt; we deal with a lot of calendar data.  For various
reasons this means we have database tables that looks something like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;+----------------------+    +----------------------+
|Calendar              |    |Event                 |
|========              |    |=====                 |
|- id                  |    |- id                  |
|- source              |    |- organization_id     |
|- organization_id     |    |- calendar_id         |
|                      |    |- organizer_id        |
+----------------------+    |- title               |
                            |- start_at            |
                            |- end_at              |
                            +----------------------+
+----------------------+    +----------------------+
|Participant           |    |Organizer             |
|===========           |    |=========             |
|- id                  |    |- id                  |
|- organization_id     |    |- organization_id     |
|- event_id            |    |- event_id            |
|- email               |    |- name                |
|- name                |    |- email               |
+----------------------+    +----------------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;(&lt;em&gt;Note this is not how it is actually setup in production.&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;Now, you can be happy or upset with the design for various reasons
but my biggest issue is how Django queries these tables for our API.  The
simplest most direct way to serialize an Event including its calendar,
organizer, and the participants cause what I call the &amp;ldquo;waterfall of doom&amp;rdquo;,
a.k.a. the N+1 problem. This topic is covered in many places throughout the
internet, for example:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://secure.phabricator.com/book/phabcontrib/article/n_plus_one/&#34;&gt;Performance: N+1 Query Problem&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/97197/what-is-the-n1-selects-problem&#34;&gt;What is the n+1 selects problem?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;so I won&amp;rsquo;t focus on it here; but basically, every request for an Event results
in 4 more requests (&lt;em&gt;the waterfall&lt;/em&gt;) for Organization, Calendar, Organizer, and
the Participants respectively.  Even worse, if I request a list of Events the
simplest code for the API will do 4 database queries for each Event in the list
(&lt;em&gt;the waterfall of doom&lt;/em&gt;). For a single event this isn&amp;rsquo;t noticeable, but if I
want to serialize all events in a calendar, it is a big performance problem.&lt;/p&gt;

&lt;p&gt;We use &lt;a href=&#34;http://www.django-rest-framework.org/&#34;&gt;Django Rest Framework&lt;/a&gt;, so the
simple (and bad) Event serialization described above looks something like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class EventSerializer(ModelSerializer):
    organizer = OrganizerSerializer(read_only=True)
    calendar =  CalendarSerializer(read_only=True)
    organization = OrganizationSerializer(read_only=True)
    participants = ParticipantSerializer(
        source=&#39;participant_set&#39;, many=True, read_only=True)

    class Meta:
        fields = (
            &#39;id&#39;, &#39;start_at&#39;, &#39;end_at&#39;, &#39;title&#39;,
            &#39;organizer&#39;, &#39;calendar&#39;, &#39;organization&#39;, &#39;participants&#39;,
        )

class EventViewSet(ReadOnlyModelViewSet):
    queryset = Event.objects.all()
    serializer_class = EventSerializer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It is worth noting that this is clearly not how our production API is currently
designed; but, it demonstrates the core performance issue. Fortunately,
&lt;a href=&#34;https://docs.djangoproject.com/en/1.10/topics/db/optimization/#retrieve-everything-at-once-if-you-know-you-will-need-it&#34;&gt;Django provides a couple tools&lt;/a&gt;
out of the box to help us solve this performance issue: &lt;code&gt;prefetch_related&lt;/code&gt;
and  &lt;code&gt;select_related&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you are familiar with Django, then this is not news to you.  Given the setup
above you would likely jump on &lt;code&gt;select_related&lt;/code&gt; and call it a day, e.g.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class EventViewSet(ReadOnlyModelViewSet):
    queryset = Event.objects.select_related(
        &#39;organizer&#39;, &#39;calendar&#39;, &#39;participant_set&#39;).all()
    serializer_class = EventSerializer
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In fact, for a short time, this optimization worked for us; but a problem pops
up when the database starts getting big.  The above ViewSet will generate SQL
like this&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT COUNT(*) FROM (
    SELECT DISTINCT ON (
        e.start_at, e.end_at, e.title
        )
        e.*,
        o.*
    FROM event e
        LEFT OUTER JOIN participant p
            ON ( e.id = p.event_id )
        LEFT OUTER JOIN organizer o
            ON ( e.id = o.event_id )
    WHERE (
        e.calendar_id =  5051
        AND e.start_at &amp;lt;= &#39;2016-12-02 00:00:00+00:00&#39;
        AND e.end_at &amp;gt;= &#39;2016-12-01 00:00:00+00:00&#39;
        AND (
            UPPER(p.email::text) = UPPER(&#39;name@example.com&#39;)
            OR UPPER(o.email::text) = UPPER(&#39;name@example.com&#39;)
        )
    )
    ORDER BY
        e.start_at ASC,
        e.end_at ASC,
        e.title ASC,
) sub;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which looks good, but here is the problem&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/42eujc1S4_syWt2a64akZ28YxvBPofo2vHeJLWE_OwwSyruc_xLZvoe2RPIsRHHM_hTqHgjT6tH1-v9U76juQ2Ik7qInTADCbMpqFJC-hIswhJt8u36Y4dXdla9ffOy1GkI2OFxO_OXsSlDpjJJEUJs3cew54TEiLOT7zupFmzyA2LVGf8aYExjzKpYIQLdctzPPk59ecdNlhtscFZNl6Q47vHKyyq04CYOaHHop-KrOJMVAsorJbb8EYeVnrcanmM6Iowy5hkfRa2NuUMEvzOnd3hoZPxSVxNRTZukULPTsgCmTw5-cbUQi_9T3zrIRpypqEzjH1WaCaBeTtzBJlncNEC6MYM1GFP7e68qBMZpuAIzO-i4i43dTY5n3IfpY--ESYXtOp6xhKEWvOIDUtYGxXNow7HOWuluiOC0rWffxdDUWgDgv_CAAmVBCIWMvUXPOinz2N18V28-SeMsLG7Lsi6CiMu4ZfJGA44m6vY1CAi_3pQTXdB7xr1YU05Bzu4qyJiV0hRThIrC3GIj4nPo0wxI1PYneHY-AnAXmxVpMerQkKdtYpglv3DuJQraoxJg9Iu_nxoQO73fi6C7ZXisrVj-wCZE6_HsRYlsm5Yp_kH365gsX=w1440-h476-no&#34; alt=&#34;Example Performance with select_related&#34; /&gt;&lt;/p&gt;

&lt;p&gt;21 seconds querying the events table in our real life production database! Why
oh why is this happening?  A quick &lt;code&gt;EXPLAIN ANALYZE&lt;/code&gt; shows that Postgres is
doing sequential scan on &lt;code&gt;participant&lt;/code&gt; and &lt;code&gt;organizer&lt;/code&gt;.  Now, I am not a DBA so
I can&amp;rsquo;t fully explain why this is happening, but I do have some ideas about how
to avoid the scan.  Here is the fastest query I could make in SQL&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT COUNT(*) FROM (
    SELECT DISTINCT ON (
        e.start, e.end, e.title,
        )
        e.*,
        o.*
    FROM event e
        LEFT OUTER JOIN (
            SELECT * FROM participant
            WHERE
                participant.organization_id = 9776
                AND UPPER(participant.email::text) = UPPER(&#39;name@example.com&#39;)
        )
        p ON ( e.id = p.event_id )
        LEFT OUTER JOIN (
            SELECT * FROM organizer
            WHERE UPPER(organizer.email::text) = UPPER(&#39;name@example.com&#39;)
        ) o ON ( e.id = o.event_id )
    WHERE (
        e.organization_id =  9776
        AND e.start_at &amp;lt;= &#39;2016-12-02 00:00:00+00:00&#39;
        AND e.end_at &amp;gt;= &#39;2016-12-01 00:00:00+00:00&#39;
        AND (p.id IS NOT NULL OR o.id IS NOT NULL) -- User filter applied in subselect joins
    )
    ORDER BY
        e.start_at ASC,
        e.end_at ASC,
        e.title ASC
) sub;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This query is fast in production, but &amp;hellip; it can not be produced using the
Django ORM. Here, I explicitly made subqueries on indexed columns to speed it
up. In a future post, I promise to discuss how and when we use raw SQL so speed
up some of our requests. But, for the general CRUD API endpoint, I want to use
the ORM so that I can leverage the great filter framework provided by Django
Rest Framework. After a lot of tinkering, the best ORM compatible query I could
design is this&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;SELECT COUNT(*) FROM (
    SELECT DISTINCT ON (
        e.start, e.end, e.title
        )
        e.*,
        o.*
    FROM event e
        LEFT OUTER JOIN participant p
            ON ( e.id = p.event_id )
        LEFT OUTER JOIN organizer o
            ON ( e.id = o.event_id )
    WHERE (
        e.organization_id =  9776
        AND e.start_at &amp;lt;= &#39;2016-12-02 00:00:00+00:00&#39;
        AND e.end_at &amp;gt;= &#39;2016-12-01 00:00:00+00:00&#39;
        AND (
            UPPER(p.email::text) = UPPER(&#39;name@example.com&#39;)
            OR UPPER(o.email::text) = UPPER(&#39;name@example.com&#39;)
        )
        AND (p.organization_id = 9776 OR p.organization_id IS NULL)
    )
    ORDER BY
        e.start_at ASC,
        e.end_at ASC,
        e.title ASC
) sub;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For specific API queries, we actually using a similar query in production. But,
I couldn&amp;rsquo;t find a good general solution until I realized that trying to do
everything at once might actually be too much to ask for, especially since the
API enforces relatively small pages. While reading through the documentation to
hunt down another bug, I &lt;a href=&#34;https://docs.djangoproject.com/en/1.10/ref/models/querysets/#prefetch-related&#34;&gt;re-read the docs&lt;/a&gt;
son &lt;code&gt;prefetch_related&lt;/code&gt; and it occurred to me that I could guarantee
exactly 4 fast queries to the db on every API call instead of one sporadically
slow query.  With the following small change to our viewset definition, our
slowest API call is an order of magnitude faster than the previous call with the
&lt;code&gt;select_related&lt;/code&gt; (although there is room for improvement)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/pUhp4eFa0T-hdtkHBUiowhLMhz9VTbWjt7L7iorDjvGpvybFl32Y-hK70ndpgmiCm33cHhsKqFSaj99IERwt28YHe-Ndb9jutVHWHCoVq0hjiCnws7HhgqGh5z1v40yZbihCgxDTf8Wf-dAXwyAYQ0RttclPPNIE4sLMUioLnkosHWYJ_if9VVjJ6xOPZACDGC70MijYqOG2TfYyKNBGmExl6mFf-F2v-vkNsXHFQzs9Wm12yM0TJ7Hrx7LOrHIOcVjb3OxBveoedc_qXoG89rTb3h9xvRRDigG0bEFcEYVRi3WjNsXt3gpuXb05tOepcgpLAHG8Nfpjx0cW0axwInwVPdrxLcfEbH6SnnnFNpTQQIX3UtEDV3z0AKvpFQwfvz8lfdc2Jr3FCXJqG0AtxXV1PwbWMZ4wudtcgLogUYkxXsfzTTRlDO5p8bPZZQSdfCDCrJdp_3lj1FUK7aNZsTVN-7-lVKdCQbYXEOit01zA_Jrv3fWM5znx1OPe_kMNWQqnpOk6uRT97zAi2Boopu4fnyoLn-wDfuiw10wpfaQEFIC910tSOeD1RBn9AF5lOlbop_vYrTU5Z5VBN3izI-ug8ucJKk6YgIb9bv2epzGTDUaehdIheMYEWtoAnmcSFE3XEFR--hy1m_w-1TZAIS4ex_A0O-cQ5DgaFzKTfg=w2286-h620-no&#34; alt=&#34;Performance with prefetch_related&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In fact, the careful observer will notice that the called components in this
trace are a bit different.  Well, the &lt;code&gt;prefetch_related&lt;/code&gt; was so successful,
that the same exact query no longer qualifies for being traced in New Relic and
the average dropped by half!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://lh3.googleusercontent.com/IGNUFurt9JEPG5c_zHAO8RkML0joObxq66RPZ7ga3xrXYGpnKiNjUPMDAsGCGuEZ4nyMJz7crE5a5UuBQuMFSTl6biLTsahB6MPbbCLehVaBGhaJTQvrwvWao0049ksDScEb6l5KgqPr8kXZ75L1k3V1B2QGpzUTeV0jN3d2SWGtuHFrW6K2wNdnjGifnBP5bXcB_3MoAvj3ZAXmvmI08_o_n_UjSN0fSMAupq2tR013pB4HGYr1HXIN9-UZRc9oqjMqtu2rslGQi0FTxrOHY5xfn6lX-0Hzy8d0qKSkhrkW8GYkk0rZwRcoQonrivtCOMeXkFB2d5l45FZlW0oM2Yo0W9Zc_jFKeRnLSyX0cdbutBCoR1Q4KFxRKz0dFrPJFCvUA4gngJ_IYdkriL6uAIq1p3lHPze_1XI2reHp2qM689-ggmppmwrlyT6QVVMy5jA6xKG7hklVkmtgsm-aj8nw5tD3OkCbJLRlAJJabIxusEKHTTI5rDtXKaqkBAidWmAF1PMuEuj2wcNQvLCN93Y2Sy_zL-aY4JDpRFbmbYh2PfJarmHhibCUF3pWK79kI_uqLwmPb1a6g1UkuFQb1zA2FmXaoKD_T8wUBOUWfQ6NhKg6gCV_SYngrtMcvI3-nDmcvIALEn7xOaM32TGOAPx5igkG7QDkM2-dKHKjNA=w1260-h298-no&#34; alt=&#34;average reservations api call&#34; /&gt;&lt;/p&gt;

&lt;p&gt;I would like to caution that I believe this works so well in this case because we
limit the API page size, so, the data and query sizes in the related queries are
relatively small per request. If you need to load a lot of data at once, I can
foresee this solution being much worse than &lt;code&gt;select_related&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;TL;DR: Monitor and analyze your production queries and &lt;code&gt;prefetch_related&lt;/code&gt; can
be a great solution if you can keep the number of queries and the page size
small.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>